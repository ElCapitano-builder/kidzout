name: Event Crawler

on:
  schedule:
    - cron: '0 6,18 * * *'   # 06:00 & 18:00 UTC
  workflow_dispatch:

permissions:
  contents: write

concurrency:
  group: kidzout-crawler
  cancel-in-progress: true

jobs:
  crawl:
    runs-on: ubuntu-latest

    steps:
      - name: Checkout (full history)
        uses: actions/checkout@v4
        with:
          fetch-depth: 0
          ref: main

      - name: Setup Python
        uses: actions/setup-python@v5
        with:
          python-version: '3.11'

      - name: Install deps
        run: pip install requests beautifulsoup4 feedparser python-dateutil icalendar

      - name: Run crawler
        run: python crawler.py

      # WICHTIG: erst committen, dann rebasen, dann pushen
      - name: Commit & push changes (with rebase)
        run: |
          set -e
          git config user.name "github-actions[bot]"
          git config user.email "41898282+github-actions[bot]@users.noreply.github.com"

          # Dateien zum Commit vormerken; wenn nichts ge√§ndert, macht commit keinen Fehler
          git add data.json || true
          git commit -m "Update events data" || echo "No changes to commit"

          # Rebase auf aktuellem Remote-Stand (verhindert non-fast-forward)
          git pull --rebase origin main || true

          # Pushen (funktioniert auch, wenn es nichts Neues gab)
          git push origin HEAD:main
